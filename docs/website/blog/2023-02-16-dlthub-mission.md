---
slug: dlthub-mission
title: dltHub Mission - why we exist
authors:
  name: Matthaus Krzykowski
  title: Co-Founder & CEO at dltHub
  url: https://twitter.com/matthausk
  image_url: https://pbs.twimg.com/profile_images/642282396751130624/9ixo0Opj_400x400.jpg
tags: [dlthub, mission, dlt]
---

**dltHub Mission** 

Since 2017, the number of Python users has been increasing by millions annually. The vast majority of these people leverage Python as a tool to solve problems at work. **Our mission is to make this next generation of Python users autonomous when they create and use data in their organizations.** For this end, we are building an open source Python library called data load tool (dlt).

These Python practitioners, as we call them, use dlt in their scripts to turn messy, unstructured data into regularly updated datasets. It empowers them to create highly scalable, easy to maintain, straightforward to deploy data pipelines without having to wait for help from a data engineer. When organizations eventually bring in data engineers to help with data loading, these engineers build on their work and evolve dlt pipelines. 

We are dedicated to keeping dlt an open source project surrounded by a vibrant, engaged community. To make this sustainable, dltHub stewards dlt while also offering additional software and services that generate revenue (similar to what GitHub does with Git).

**Why does dltHub exist?** 

We believe in a world where data loading becomes a commodity. A world where hundred thousands of pipelines will be created, shared, and deployed. A world where data sets, reports, and analytics will be written and shared publicly and in private.

To achieve our mission ***to make a next generation of Python users autonomous when they create and use data in their organizations*** we need to address the requirements of both the Python practitioner and the data engineer with a minimal Python library. We also need dltHub to become the GitHub for data pipelines, to facilitate and support the ecosystem of pipeline creators and maintainers as well as the other data folks who consume and analyze the data loaded.

There are lots (+300!) of tools available for ETL. Yet, as we engaged with Python practioners in the last one and half years we found only few Python practitioner that uses traditional data ingestion tools. Only a handful had even heard of them. Very simplified, there’s two approaches in traditional data ingestion tools: There’s 1) SaaS solutions to handle the entire data loading process as well as 2) Python frameworks for software engineers. SaaS solutions do not give Python practitioners enough credit, while Python frameworks expect too much of them. In other words, there’s no “Jupyter notebook, Pandas, Numpy for data loading” that meets users needs. As millions of Python practioners are now entering organisations every year we think this should exist.
