---
sidebar_position: 8
---

# Concepts

## [Resource](./walkthroughs/create-a-pipeline.md)


A resource is a data producing method. To create a resource, we add the `@resource`decorator to a generator function

arguments:

- `name` The name of the table generated by this resource. Defaults to resource name.
- `write_disposition` How should the data be loaded at destination? Currently supported: `append`, `replace`. Defaults to `append.`
- `depends_on` You can make a resource depend on another, for example for the use case when you need to pass data from a resource to another, or for cases where you want to request field renames before the fields.

Example:

```python
@resource(name='table_name', write_disposition='replace')
def generate_rows(nr):
	for i in range(nr):
		yield {'id':i, 'example_string':'abc'}
```

To get the data of a resource, we could do

```python

for row in games():
		print row

for row in sql_source().resources.get('table_users')
		print(row)

```

## [Source](./walkthroughs/create-a-pipeline.md)

A source contains:

- API methods for authentication, pagination
- Resources are functions that produce data. Resources are to sources what endpoints are to an API.
- workflow and customisation code.

A source is the piece of code that produces data. A source might typically be built around a single api. It contains resources, which are data prod

A source contains the **resources**, `source_name().resources` produces a dictionary of resources with the name as key. This allows you to select and execute them individually or in your preferred patterns.

For example, you may want to break down a 50 table copy job into an airflow dag with high parallelism to load the data faster. To do so, you could get the list of resources as

```python
# get a list of resources' names
resource_list = sql_source().resources.keys()

#now we are able to make a pipeline for each resource
for res in resource_list:
		pipeline.run(sql_source().with_resources(res))
```




## [Pipeline](./walkthroughs/create-a-pipeline.md)

  > **üí°** Moves the data from the source to the destination, according to instructions provided
  in the schema (i.e. extracting, normalizing, and loading the data).


A pipeline is a connection to the destination. We pass sources or resources to the pipeline. We can also pass generators to the pipeline, . When the pipeline runs, the resources get executed and the data is loaded at destination.

Arguments:

`source` Source may be a dlt source, resource or a generator

`name` is the unique identifier for the pipeline. This unique identifier is used to reference the logs and the state of the pipeline.

`full_refresh` (bool) is a toggle you can use during development to avoid conflicts between pipeline states as you develop them. Full refresh will create a new dataset version with a timestamp in the name. Once you are done developing, make sure to turn it off to funnel the data back into the right dataset.

Example: This pipeline will load the data the generator `gen(10)`produces

```python
import dlt

def gen(nr):
    for i in range(nr):
        yield {'id':1}

pipeline = dlt.pipeline(destination='bigquery',dataset_name='sql_database_data')

info = pipeline.run(gen(10))

print(info)
```

A pipeline‚Äôs unique identifier is the name. If no name is given, then the pipeline takes the name from the currently executing file.

## [Schema](./customization/schema.md)

  Describes the structure of normalized data (e.g. unpacked tables, column types, etc.) and provides instructions on how the data should be processed and loaded (i.e. it tells `dlt` about the content
  of the data and how to load it into the destination).

## [Config](./customization/configuration.md)

  A set of values that are passed to the pipeline at run time (e.g. to change its behavior locally
  vs. in production).

## [Credentials](./customization/credentials.md)

  A subset of configuration whose elements are kept secret and never shared in plain text.


## [State](./customization/state.md)

> **üí°** The ‚Äústate‚Äù of a pipeline refers to metadata that we store about the data.
>
> We can use the state as a persistent python dictionary peristed between pipeline runs in which we can store any information
>
>For incremental pipelines, the state stores information about what was loaded, to know where to resume.

**What is stored in the state?**

- dlt engine configuration and inferred schema (defaults, if you do not modify them)
- variables that you choose to store
    - for example, the ‚Äúlast id‚Äù or ‚Äúlast timestamp‚Äù of a loaded data set, so we can make an incremental request.
    - or, in the case of field renames, we store the field mapping there.

**Where is state created and maintained?**

The state is stored in the `dlt_state`table at the destination and contains information about the pipeline, pipeline run (that the state belongs to) and state.
You can create the state in your python script as such

Example: Track ONE state for a single resource
```python

# Get archives from state. If state does not exist, get the default, in this case empty list []
loaded_archives = dlt.state().setdefault("archives", [])
new_archives = get_archives(url)
archives_to_be_loaded = [a for a in new_archives if a not in loaded_archives]
for archive in archives_to_be_loaded:
  yield archives

```
Example: Track multiple states for multiple search terms
```python
# get dlt state to store last values of tweets for each search term we request
last_value_cache = dlt.state().setdefault("last_value_cache", {})
# use `last_value` to initialize state on the first run of the pipeline
last_value = last_value_cache.setdefault(search_term, last_value or None)
# if we have a last value, then use it for request params. If not, then we make an unparametrised request and get all data.
if last_value:
  request_params['since_id'] = last_value
# ... get data and set the new state
# the state will be comitted atomically with the data - you do not need to yield it or do anything more
last_value_cache[search_term] = max(last_value_cache[search_term], int(last_id))

```

**What about local state?**

There is also a local state that happens during a run. This state is a temporary cache that supports passing data between resources and buffered loading. You will not interact with this state directly, and it should be cleaned up after every run.

During development, cancelling the running script may cause the state cleanup to fail, and a state conflict to occur on the next run. To resolve this, use `full_refresh=True` in your pipeline to create a new state.

It is important to be aware of this local state, as resources that share state need to be run on the same worker machine.

**How can we share state between loading pipelines?**

The state is identified by pipeline name and destination dataset. To re-use the same state, use the same pipeline name and destination.
To re-use the same local state, for example in airflow, where separate tasks may end up running on separate machines, you can force 2 resources to run on the same machine by putting them in a single task.