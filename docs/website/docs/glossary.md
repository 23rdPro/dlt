---
sidebar_position: 6
---

# Glossary

## [Resource](./walkthroughs/create-a-pipeline.md)

  > **ðŸ’¡** If the source is an API, then a resource is an endpoint in that API. If the source is a
  spreadsheet, then a resource is a tab in that spreadsheet. If the source is a database,
  then a resource is a table in that database. A source is organized into one or more resources.



A resource is a data producing method. To create a resource, we add the `@resource`decorator to a generator function

arguments:

- `name` The name of the table generated by this resource. Defaults to resource name.
- `write_disposition` How should the data be loaded at destination? Currently supported: `append`, `replace`. Defaults to `append.`
- `depends_on` You can make a resource depend on another, for example for the use case when you need to pass data from a resource to another, or for cases where you want to request field renames before the fields.

Example:

```python
@resource(name='table_name', write_disposition='replace')
def generate_rows(nr):
	for i in range(nr):
		yield {'id':i, 'example_string':'abc'}
```

To get the data of a resource, we could do

```python

for row in games():
		print row

for row in sql_source().resources.get('table_users')
		print(row)

```

## [Source](./walkthroughs/create-a-pipeline.md)

  > **ðŸ’¡** If endpoints in an API are the resources, then the API is the source. If tabs in a spreadsheet
  are the resources, then the source is the spreadsheet. If tables in a database are the resources,
  then the source is the database. A source is organized into one or more resources.



A dlt Source is the code that handles getting data from an api.

A source contains:

- API methods for authentication, pagination
- Resources are functions that produce data. Resources are to sources what endpoints are to an API.
- workflow and customisation code.

A source is the piece of code that produces data. A source might typically be built around a single api. It contains resources, which are data prod

A source contains the **resources**, `source_name().resources` produces a dictionary of resources with the name as key. This allows you to select and execute them individually or in your preferred patterns.

For example, you may want to break down a 50 table copy job into an airflow dag with high parallelism to load the data faster. To do so, you could get the list of resources as

```python
# get a list of resources' names
resource_list = sql_source().resources.keys()

#now we are able to make a pipeline for each resource
for res in resource_list:
		pipeline.run(sql_source().with_resources(res))
```


## [Destination](./walkthroughs/create-a-pipeline.md)

  The data store where data from the source is loaded (e.g. Google BigQuery).

## [Pipeline](./walkthroughs/create-a-pipeline.md)

  > **ðŸ’¡** Moves the data from the source to the destination, according to instructions provided
  in the schema (i.e. extracting, normalizing, and loading the data).


A pipeline is a connection to the destination. We pass sources or resources to the pipeline. We can also pass generators to the pipeline, . When the pipeline runs, the resources get executed and the data is loaded at destination.

Arguments:

`source` Source may be a dlt source, resource or a generator

`name` is the unique identifier for the pipeline. This unique identifier is used to reference the logs and the state of the pipeline.

`full_refresh` (bool) is a toggle you can use during development to avoid conflicts between pipeline states as you develop them. Full refresh will create a new dataset version with a timestamp in the name. Once you are done developing, make sure to turn it off to funnel the data back into the right dataset.

Example: This pipeline will load the data the generator `gen(10)`produces

```python
import dlt

def gen(nr):
    for i in range(nr):
        yield {'id':1}

pipeline = dlt.pipeline(destination='bigquery',dataset_name='sql_database_data')

info = pipeline.run(gen(10))

print(info)
```

A pipelineâ€™s unique identifier is the name. If no name is given, then the pipeline takes the name of the source or generator you pass to it.

## [Schema](./customization/schema.md)

  Describes the structure of normalized data (e.g. unpacked tables, column types, etc.) and provides instructions on how the data should be processed and loaded (i.e. it tells `dlt` about the content
  of the data and how to load it into the destination).

## [Config](./customization/configuration.md)

  A set of values that are passed to the pipeline at run time (e.g. to change its behavior locally
  vs. in production).

## [Credentials](./customization/credentials.md)

  A subset of configuration whose elements are kept secret and never shared in plain text.